# Chapter 5: Assessment Design — Measuring Genuine Claude Proficiency

## Learning Objectives

By the end of this chapter, you will be able to:

- Distinguish between assessments that measure genuine Claude competency and those that measure recall
- Design formative and summative assessments appropriate for different training contexts
- Build rubrics for evaluating prompt quality and output verification skill
- Develop competency benchmarks that define what "good" looks like at beginner, intermediate, and advanced levels
- Create practical skill evaluations that are defensible, scalable, and meaningful to learners

---

## The Assessment Problem in AI Training

Most Claude training programs that include assessment use the wrong instruments. They test what is easy to measure — knowledge about Claude — rather than what matters — the ability to use Claude effectively for real work.

This is not a trivial problem. When assessment instruments do not match actual competencies, the program generates false confidence (learners believe they are prepared when they are not) and misses the feedback signal needed to improve training content. Organizations that invest in training without valid assessment have no way to know whether that investment produced anything.

The assessment challenge in AI training is compounded by the fact that Claude outputs are not binary. Unlike a software certification where a configuration is either correct or not, prompt quality exists on a spectrum. Assessment design must account for this inherent subjectivity and build rubrics that make evaluations defensible and consistent across raters.

---

## Competency vs. Knowledge: The Core Distinction

Assessment instruments fall into two broad categories:

**Knowledge assessments** test what learners know: definitions, facts, concepts, principles. Multiple-choice questions, true/false questions, and short-answer questions about Claude's capabilities are knowledge assessments. They are easy to create and score but have weak predictive validity for real-world Claude performance.

**Competency assessments** test what learners can do: write an effective prompt, evaluate an output, identify a hallucination, adapt content for a new use case. These are harder to design and score but have strong predictive validity for actual adoption and effective use.

A well-designed Claude assessment program uses knowledge assessments sparingly — for concepts where accurate understanding is essential and where the knowledge itself predicts behavior — and uses competency assessments as the primary measurement instrument.

**Rule of thumb:** If a learner could answer the assessment question correctly without ever opening Claude, it is a knowledge question, not a competency question.

---

## The Three-Level Competency Framework

Before building assessments, define what competency looks like at each level. The following framework provides a starting point that can be adapted for your organizational context.

### Level 1: Functional (Beginner)

A functional user can:

- Navigate the Claude interface without assistance
- Write prompts that produce relevant (if imperfect) outputs for straightforward tasks
- Identify obviously poor outputs and recognize that improvement is needed
- Apply basic context and formatting to standard requests
- Understand what types of outputs require verification

**Assessment focus:** Can they complete a defined task with Claude? Can they identify a bad output?

### Level 2: Proficient (Intermediate)

A proficient user can:

- Consistently write prompts that produce high-quality outputs for role-specific tasks
- Diagnose prompt failures and improve them systematically
- Chain prompts to complete multi-step tasks
- Verify outputs appropriate to risk level
- Adapt prompts for different audiences or purposes

**Assessment focus:** Can they produce consistently high-quality outputs? Can they fix broken prompts?

### Level 3: Expert (Advanced)

An expert user can:

- Design prompt systems for complex organizational workflows
- Train and coach others on effective Claude use
- Evaluate Claude outputs against domain-specific quality standards
- Identify use cases where Claude is and is not appropriate
- Build and document prompt pattern libraries for team use

**Assessment focus:** Can they build systems, teach others, and evaluate complex outputs against domain standards?

---

## Assessment Instruments by Type

### Instrument 1: The Prompt Production Task

**Description:** Give learners a task brief and ask them to write a prompt. Score the prompt using a rubric before seeing the output, and score the output separately.

**Example Task Brief:**
"You manage a customer success team and have just received a 2,500-word transcript from a difficult customer call. Write a prompt that will give you a useful summary for your next conversation with this customer's account manager."

**Prompt Rubric (4-point scale per dimension):**

| Dimension | 1 — Absent | 2 — Partial | 3 — Present | 4 — Excellent |
|-----------|-----------|-------------|-------------|---------------|
| **Role specification** | No role assigned | Vague role | Clear role assigned | Role with relevant context |
| **Task instruction** | No clear task | Action unclear | Clear action verb | Clear, specific, bounded action |
| **Context provided** | No context | Minimal context | Adequate context | Rich, relevant context |
| **Output specification** | No format specified | Vague format | Format specified | Format with purpose-fit constraints |
| **Appropriate length** | Severely under- or over-specified | Slightly off | Appropriate | Optimally scoped |

**Scoring:** 16–20 = Proficient; 10–15 = Functional with coaching needed; <10 = Needs foundational rework

**Variations by level:** At Level 1, score only on task completion (did they produce a relevant output?). At Level 2, use the full rubric. At Level 3, add a dimension for prompt efficiency (was the prompt no longer than it needed to be?).

---

### Instrument 2: The Output Evaluation Task

**Description:** Present learners with a prompt and three outputs — one high-quality, one mediocre, and one poor — and ask them to rank and justify.

**Why this works:** Output evaluation is a learnable skill that directly predicts real-world use. Users who cannot distinguish good outputs from poor ones will use whatever Claude first produces without critical review.

**Example Setup:**

*Prompt provided:* "Write a subject line for a cold outreach email to a VP of Operations at a manufacturing company about an AI-powered inventory management system."

*Output A:* "Transform Your Operations with AI"
*Output B:* "AI Inventory Management — Worth 15 Minutes?"
*Output C:* "Following up on your inventory challenges"

*Assessment task:* Rank the three outputs from best to worst. For each, write 1–2 sentences explaining your ranking. For the lowest-ranked output, write an improved version.

**Scoring rubric:**

- Accurate ranking with clear reasoning: 3 points
- Ranking correct but reasoning unclear: 2 points
- Ranking partially correct: 1 point
- Improvement of weak output: scored separately on quality (0–3 points)

---

### Instrument 3: The Prompt Debugging Task

**Description:** Present a prompt and a clearly unsatisfactory output. Ask learners to identify what is wrong with the prompt and produce an improved version.

**Example:**

*Original Prompt:* "Tell me about our market."

*Output received:* (A generic paragraph about market research methodology)

*Assessment task:*
1. Identify at least two specific problems with the original prompt
2. Write an improved prompt that would produce a more useful output
3. Explain the specific changes you made and why

**Scoring:**

- Problem identification (0–4): 2 points per correctly identified problem, up to 2 problems
- Improved prompt quality (0–4): scored on RICO rubric
- Explanation quality (0–2): clear reasoning earns full credit

---

### Instrument 4: The Workflow Design Task

**For Level 3 / Expert assessment only**

**Description:** Present a complex organizational scenario and ask learners to design a Claude-integrated workflow.

**Example Scenario:** "Your legal team reviews 20–30 vendor contracts per month. Each review currently takes 3–4 hours per attorney. Design a Claude-supported workflow that reduces attorney time while maintaining review quality and appropriate oversight."

**Assessment criteria:**

- Identifies appropriate tasks for Claude vs. attorney judgment
- Designs a prompt sequence (not just a single prompt) for the workflow
- Addresses verification and oversight requirements
- Identifies risks and mitigation strategies
- Produces at least one sample prompt for the highest-value task in the workflow

This is a complex, open-ended task that requires expert-level judgment. Score it holistically (0–10) with a calibration guide to align rater scores.

---

## Formative vs. Summative Assessment

### Formative Assessment

Formative assessment happens during training and informs both learner development and trainer adjustment. It is low stakes, frequent, and feedback-rich.

**Practical formative instruments:**

- **Exit tickets:** 2–3 questions at the end of each module — "What is the most useful prompt pattern you learned today?" "What are you still unsure about?"
- **Peer review:** Learners exchange prompts and give each other feedback using a provided checklist
- **In-session demonstrations:** Trainer observes learners running prompts and provides real-time coaching
- **Self-assessment:** Learners rate their own confidence against specific competencies before and after training

Formative data should inform real-time facilitation adjustments. If exit tickets reveal consistent confusion about a concept, revisit it in the next session — do not move on.

### Summative Assessment

Summative assessment happens at program completion and measures final competency. It is higher stakes and produces records suitable for reporting.

**Design principles for summative assessments:**

1. **Task authenticity** — the assessment task should resemble actual work, not a test scenario
2. **Rubric clarity** — every scoring dimension should be defined well enough that two independent raters would score the same response the same way
3. **Calibration** — raters should be calibrated against sample responses before scoring live assessments
4. **Feedback loop** — learners should receive specific, actionable feedback on summative results, not just scores

---

## Designing Rubrics for Subjectivity

The hardest part of Claude assessment is scoring outputs where quality is genuinely subjective. The following practices make rubrics defensible:

**Anchor responses:** For each score level, provide at least one sample response that exemplifies that score. Raters calibrate against the anchors, not against each other's intuition.

**Behavioral language:** Define rubric levels in behavioral terms ("The prompt includes a specific role assignment that is relevant to the task domain") rather than evaluative terms ("The prompt is good").

**Holistic + analytic hybrid:** Use analytic rubrics (dimension-by-dimension scores) for diagnostic feedback. Use holistic rubrics (overall score) for efficient summative scoring of high-volume assessments. Train raters on both.

**Inter-rater reliability checks:** On any rubric used for certification or formal evaluation, have at least 10% of assessments scored by two independent raters. Calculate inter-rater reliability. If correlation falls below 0.80, the rubric needs revision.

---

## Competency Certification Design

If your organization uses Claude proficiency certification, the certification architecture should reflect the three-level competency framework.

**Recommended certification structure:**

| Level | Name | Assessment | Time to Complete |
|-------|------|-----------|-----------------|
| 1 | Claude Practitioner | Prompt Production Task (2 prompts) | 30 minutes |
| 2 | Claude Proficient | Prompt Production (3), Output Evaluation (3), Prompt Debugging (2) | 75 minutes |
| 3 | Claude Expert | Full portfolio including Workflow Design Task | 90 minutes + portfolio review |

**Certification validity:** Set an expiration period (typically 12–18 months for AI tools) that requires recertification. Claude's capabilities evolve; competencies validated against 2023 Claude may not reflect effective use of current Claude.

---

## Exercise 1: Rubric Design Workshop

**Time:** 40 minutes
**Format:** Teams of 3–4
**Materials:** Four sample outputs for the same prompt task

Each team receives four Claude outputs generated from the same prompt — representing roughly excellent, good, mediocre, and poor quality. Without a rubric, teams first discuss: what makes the best output best? What makes the worst output worst?

From this discussion, teams draft a 4-dimension rubric for evaluating outputs of this type. They then apply their rubric to all four outputs and calculate scores.

Debrief: Do the scores match your intuitive ranking? If not, what does the rubric miss? How would you revise it?

---

## Exercise 2: Assessment Instrument Development

**Time:** 45 minutes
**Format:** Individual
**Materials:** Competency framework, assessment instrument template

Select one competency from the Level 2 (Proficient) framework relevant to your training context. Design a complete assessment instrument for that competency, including:

- Task brief (the instructions the learner receives)
- A rubric with at least 4 dimensions and 4 levels per dimension
- Two anchor responses (one at Level 2, one at Level 4)
- Scoring guide indicating pass/fail threshold

Exchange with a partner. Could they score a response using your rubric without additional explanation? Identify any dimensions that require clarification.

---

## Key Takeaways

- Knowledge assessments measure what learners know; competency assessments measure what they can do. AI training programs should rely primarily on competency assessment.
- The three-level competency framework — Functional, Proficient, Expert — provides a developmental progression that guides both assessment design and career pathing for AI capability.
- The four primary assessment instruments — Prompt Production, Output Evaluation, Prompt Debugging, and Workflow Design — cover the full competency spectrum and can be combined into certifications.
- Rubric quality determines assessment reliability. Behavioral language, anchor responses, and inter-rater calibration are non-negotiable for defensible scoring.
- Formative assessment informs training delivery in real time; summative assessment produces records. Both are necessary for a complete assessment program.
- Certification validity periods must account for Claude's evolution. Skills validated against an older version of Claude may not transfer to current capabilities.

---

*Next: Chapter 6 — Quality Assurance for Training*
