# Chapter 2: Prompt Engineering Pedagogy — Teaching Others to Prompt Effectively

## Learning Objectives

By the end of this chapter, you will be able to:

- Distinguish between using prompts effectively yourself and teaching others to build prompt intuition
- Apply a structured pedagogical framework for introducing prompt engineering to new learners
- Identify and address the five most common prompt anti-patterns that emerge in novice users
- Design practice sequences that build learner confidence progressively
- Facilitate productive prompt debugging sessions without creating dependency on the trainer

---

## The Gap Between Doing and Teaching

Most trainers who become expert prompt engineers do so by trial and error over months of daily use. They internalize patterns without being able to articulate them. When asked to teach prompting, they demonstrate their own best prompts — which learners cannot replicate because they don't understand the underlying reasoning.

This is the central challenge of prompt engineering pedagogy: **the skill gap is not about knowledge, it is about internalized judgment.** Teaching someone to prompt effectively is more like teaching someone to write clearly than teaching them to use software. You cannot simply show the correct answer and expect it to transfer.

This chapter provides a pedagogical architecture — a teaching system — that builds transferable prompt intuition rather than demonstrating individual prompt examples.

---

## A Framework for Understanding Prompts

Before you can teach prompting, you need a vocabulary that learners can internalize. The following framework treats every prompt as having four structural elements:

### The RICO Framework

**R — Role**
Who is Claude being asked to be? Assigning a role gives Claude context about the perspective, tone, and domain knowledge it should draw on. "You are a senior financial analyst" is fundamentally different from "You are a startup founder explaining finances to a non-expert investor."

**I — Instruction**
What is Claude being asked to do? The instruction is the core action verb: write, analyze, summarize, compare, extract, explain, critique, generate. The precision of the instruction determines how directly Claude can execute the task.

**C — Context**
What does Claude need to know to do this well? Context includes the audience, the purpose, any constraints, background information, and the situation the output will be used in. Most novice prompts fail at this layer — they give Claude an instruction without the context needed to tailor the output.

**O — Output**
What does the final response need to look like? Format, length, structure, tone, and any specific requirements. Specifying output prevents the default Claude response (comprehensive, moderately formal, often longer than needed) from overriding what the task actually requires.

Teach RICO as a diagnostic tool, not as a template. Experienced prompters do not write out all four elements for every prompt — they intuitively check whether each element is adequately addressed. That intuition is what you are building.

---

## The Novice Prompt Development Arc

Novice users typically move through a predictable sequence as they develop prompt skill. Understanding this arc lets you design training that meets learners where they are rather than projecting expert-level thinking onto beginners.

### Stage 1: The Naive Query (Weeks 1–2 of use)

Novices interact with Claude the same way they use a search engine: short, keyword-driven queries with no context. "Write a sales email." "Summarize this document." "What is a good way to handle this?"

Outputs at this stage are generic. The novice often concludes that Claude "isn't that useful" because the outputs do not match their actual needs — even though the problem is the prompts, not Claude.

**Training intervention:** Show learners the same task with a naive prompt versus a context-rich prompt, side by side. Do not tell them which is better; ask them to evaluate both. The evaluation exercise builds discrimination before technique.

### Stage 2: Over-Engineering (Weeks 3–8)

Once learners understand that context improves outputs, many overcorrect. They write 500-word prompts for simple tasks. They add unnecessary constraints. They specify the output format in microscopic detail and then override Claude's judgment on things that would have been better left to it.

Over-engineering produces cluttered, inconsistent outputs. Learners at this stage may become frustrated because their "better" prompts seem to perform worse.

**Training intervention:** Teach the minimum effective prompt principle: include what Claude needs to know; exclude everything else. Exercises that challenge learners to cut prompts in half without losing quality are effective here.

### Stage 3: Pattern Development (Months 2–4)

With experience, learners start to recognize that certain prompt structures reliably produce certain output types. They develop a personal library of patterns — role definitions that work for specific use cases, output format specifications that produce the structure they want, context setups that consistently get useful outputs.

**Training intervention:** Make pattern development explicit. Have learners document and name their best prompt patterns. Sharing pattern libraries across teams creates a collective intelligence that accelerates individual development.

### Stage 4: Adaptive Prompting (Months 4+)

Expert prompters develop the ability to read a failing output and diagnose the prompt problem. They can mid-conversation reframe, ask Claude to critique its own output, chain prompts for complex tasks, and dynamically adjust their approach based on what Claude produces.

**Training intervention:** Case-based debugging sessions. Present outputs and ask learners to identify the prompt flaw and fix it. This is the highest-leverage teaching activity for advanced learners.

---

## The Five Common Prompt Anti-Patterns

Every trainer who has watched novices use Claude will recognize these patterns. Teaching learners to identify and avoid them is one of the highest-impact things you can do.

### Anti-Pattern 1: The Vague Instruction

**Example:** "Help me with this report."

**What happens:** Claude asks clarifying questions or produces a generic response that is useful to no one.

**Fix:** Specify the action. "Review this report and identify the three weakest arguments. For each, explain why it's weak and suggest a stronger alternative framing."

**Teaching approach:** Collect real examples of vague prompts from your organization. Ask learners to rewrite them with specific instructions before revealing what Claude actually produced.

### Anti-Pattern 2: The Implicit Context

**Example:** "Write an email to John about the project delay."

**What happens:** Claude writes a generic professional email with placeholder-level content because it has no idea who John is, what the project is, why it is delayed, or what outcome the sender is trying to achieve.

**Fix:** "Write an email to John, our lead developer, explaining that the Q3 dashboard integration will be delayed by two weeks due to an API dependency issue on the vendor side. Tone should be transparent but not alarming. We want to maintain trust and set a clear revised timeline of September 28."

**Teaching approach:** Use the "what would a colleague need to know?" test. Ask learners to write down everything a new hire on their team would need to know to complete the task — that's the context Claude needs.

### Anti-Pattern 3: The Open-Ended Format Request

**Example:** "Give me some ideas for improving customer onboarding."

**What happens:** Claude generates a brainstorm that is long, comprehensive, and difficult to use because it lacks the structure needed to prioritize or act on.

**Fix:** "Give me 5 specific, actionable ideas for improving B2B SaaS customer onboarding. For each idea, include: (1) what changes, (2) who owns the change, and (3) the expected impact. Format as a numbered list."

**Teaching approach:** Show learners the difference between an open-ended output and a structured output for the same task. Ask: which one can you act on immediately?

### Anti-Pattern 4: The Leading Prompt

**Example:** "Don't you think that our pricing strategy is the best approach?"

**What happens:** Claude confirms the premise. Because it is designed to be helpful and the prompt expresses a clear preference, Claude will tend to agree and support — even if the honest analysis would be more critical.

**Fix:** "Analyze our pricing strategy from the perspective of a competitor trying to undercut us. What are the three most exploitable weaknesses?"

**Teaching approach:** Explain the concept of confirmation bias in prompting. Claude is not immune to leading questions. To get useful critical analysis, learners must explicitly request critique or adopt adversarial framing.

### Anti-Pattern 5: The One-Shot Complex Task

**Example:** "Research our industry, identify our competitive position, develop a three-year strategic plan, and draft a board presentation."

**What happens:** Claude attempts everything and does none of it well. The output is surface-level across the board because real depth on any one of those tasks would require an entire conversation.

**Fix:** Chain the tasks. Start with competitive landscape analysis. Review and validate. Move to strategic option generation. Review and select. Then develop the plan. Then draft the presentation. Each stage builds on and validates the previous.

**Teaching approach:** Introduce prompt chaining as a skill. Complex tasks are almost always better handled as a series of focused conversations than as a single mega-prompt.

---

## Teaching Prompt Debugging

The ability to look at a failing output and diagnose the prompt is what separates intermediate from advanced users. This skill is hard to develop through instruction alone — it requires structured practice.

### The DIAG Protocol for Prompt Debugging

When a prompt produces an unsatisfactory output, walk learners through four diagnostic questions:

1. **Direction** — Was the instruction clear and specific enough?
2. **Information** — Did Claude have the context it needed to do this well?
3. **Assumptions** — Did Claude fill in assumptions you didn't intend?
4. **Goal** — Does Claude know what a good output actually looks like?

Most prompt failures are traceable to one of these four dimensions. Teaching learners to diagnose systematically is more valuable than teaching them to memorize specific fixes.

---

## Exercise 1: The Side-by-Side Prompt Comparison

**Time:** 25 minutes
**Format:** Individual, then group debrief
**Materials:** Access to Claude, a set of 5 weak prompts (prepared in advance)

Distribute a set of weak prompts — drawn from real organizational tasks if possible. Ask each participant to:

1. Submit the original weak prompt to Claude and note the output quality
2. Rewrite the prompt using the RICO framework
3. Submit the rewritten prompt and compare outputs

Debrief: What elements did you add? What did you cut? What surprised you about the difference?

This exercise creates a visceral before/after experience that abstract instruction cannot replicate.

---

## Exercise 2: Prompt Debugging Workshop

**Time:** 40 minutes
**Format:** Teams of 3
**Materials:** A set of 6 "broken" prompt + output pairs

Prepare 6 examples where a prompt produced a clearly unsatisfactory output. For each example, provide the prompt and the output. Do not provide the improved prompt.

Teams must:

1. Diagnose the failure using the DIAG protocol
2. Write an improved prompt
3. Submit and evaluate the improved output
4. Prepare a 2-minute explanation of what was wrong and what they fixed

Debrief as a group. Multiple teams will fix the same prompts differently — and both fixes may work. This is an important lesson: there is no single correct prompt, only prompts that are better or worse for the specific task.

---

## Designing Practice Sequences for Your Learners

The sequence in which you introduce prompt concepts matters. The following sequence has proven effective across diverse learner groups:

**Session 1 — Exploration (30–45 min)**
Learners use Claude for tasks they choose, with no instruction on prompting. This surfaces their baseline intuitions and the anti-patterns they naturally fall into. You will use these as teaching cases in later sessions.

**Session 2 — Diagnosis (30 min)**
Introduce the RICO framework as a diagnostic tool. Ask learners to analyze three of their own prompts from Session 1 using RICO. What was missing?

**Session 3 — Reconstruction (45 min)**
Learners rewrite their weakest Session 1 prompts using RICO. Compare outputs. Debrief on what changed.

**Session 4 — Pattern Library (30 min)**
Teams share their best prompts from Session 3. Collectively build a prompt pattern library for your specific organizational context.

**Session 5 — Debugging (45 min)**
Introduce the DIAG protocol. Work through the debugging exercise.

This sequence takes approximately 3–4 hours of total contact time and produces learners who can reliably write better prompts and diagnose failures — the two capabilities that actually matter for sustainable adoption.

---

## Key Takeaways

- Teaching prompt engineering is fundamentally different from demonstrating it. The goal is transferable judgment, not copied examples.
- The RICO framework (Role, Instruction, Context, Output) gives learners a diagnostic vocabulary that works across any use case.
- Novice users move through a predictable development arc: naive queries, over-engineering, pattern development, and adaptive prompting. Training design should match the learner's current stage.
- The five core anti-patterns — vague instruction, implicit context, open-ended format, leading prompt, one-shot complexity — account for the majority of novice failures and are teachable in a single session.
- Prompt debugging is the highest-leverage skill for advanced learners. Teach the DIAG protocol explicitly and practice it with real failing examples.
- Practice sequences matter. Exploration before instruction surfaces real anti-patterns; diagnosis before reconstruction creates insight rather than compliance.

---

*Next: Chapter 3 — Workshop Design and Facilitation*
